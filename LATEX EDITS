Important things to put in the report.
1. Justify selection of algortihms
2. Correctness proofs on code
3. Complexity analysis of overall algorithm
4. Justify efficiency tradeoffs in light of performance
5. Document all iterations and why that path was chosen
6. Appendices must include
  i. Testing/Correctness/Verification (1 section)
  ii. Runtime Complexity and Walltime (1 section)
  iii. Performance (according to the aforementioned criterion)
  iv. ustification for the algorithm selected in your final implementation out of the set of implementations you tested
7. explore the problem and solution space, and report any ideas, interesting insights, and rationale you uncover for tackling the problem
8. Any useful figures, plots, tables, or analyses are allowed to illustrate your level of depth at which you thought about the problem and your solutions.
9. Make sure all code is well-commented

FGSM algorithm
Justification - The Fast Gradient Sign Method (FGSM) is a method for generating adversial attacks that is simple yet effective.  "FGSM is a single step
attack, ie.. the perturbation is added in a single step instead of adding it over a loop (Iterative attack)" (Breaking Deep Learning with Adversarial
Examples Using Tensorflow, 2018). The method works by calculating the gradient of the loss with respect to the original image.  The algorithm then adds a 
perturbation to to the image based on the gradient's sign.  The algorithm modifies the image into its adversial version by making small perturbations.  
This allows the adversial image to look very similar to the original image while still fooling the computer.  The way this is accomplished is by finding 
how much each pixel contributes to the loss.  This is a quick process because the algorithm will follow the chain rule in finding pixels that contribute 
to maximum loss.  The chain rule breaks the computation into smaller parts, making the calculation of the gradient more accurate and efficient.  The first
step in this method is to feed the model input data and then calculate the loss.  The gradient with respect to the loss is then calculated.  From this 
gradient a perturbation is added in order to generate the new adversial model.  

We chose this method for many reasons.  The first reason is the computational efficiency of the FGSM method.  It
can generate adversial attacks on a model quickly.  This method is also easy to implement because it requires a calculation of the gradient of the loss
and then addition or subtraction of perturbations based on the sign of the gradient.  Another reason we chose to implement this method is due to its 
adaptability.  The FGSM method can be used for any machine learning model and therefore we knew it would be able to work on the binary image classifier.
The method has been proven very effective on a wide range of models.  Overall, the reason this model was selected was due to its basic implementation,
computational efficiency, and effectiveness.


Spatial-Transformation Based Attack
Justification - The Spatial-Transformation Based Attack (STBA) is an adversial attack that applies a spatial transformation to the input data in order to
create an adversial image.  "All the existing approaches directly modify pixels, but we aim to smoothly change the geometry of the scene while keeping the
original appearance, producing more perceptually realistic adversarial examples" ((Xiao et al., n.d.)).  The goal of this attack is to make the change
unrecognizable to the human eye, but still make the classifier misclassify the image.  The first step of STBA is to select a transformation function.

Possible transformations include rotating, scaling, or shearing the image.  These transformations are applied to the input and an output is produced with
features to trick the model.  

There are multiple reasons we chose this method.  The first reason is that that this model can be applied to any image without knowledge of the model.  The
attack applies a transformation until the classifier is fooled.  This means the algorthm requires much less learning and can trick the binary classifier
with more ease compared to other methods.  Another reason this model is very useful is it can be applied in a way to target specific labels that
are supposed to be misclassified.  This is useful because the only two labels were grass or dandelions, so the model could be trained appropriately
to model grass as dandelions and dandelions as grass.  You can modify the input image to look like something else and because there were only two options
this is not a hard task to implement. The ways we chose to implement were through rotating, scaling, and shearing.  Rotating the image by a certain angle
can create different featurs such as lines or curves.  Scaling the image involves expanding or shrinking dimensions along the x or y axis.  This can alter
certain defining features by making them narrower or shorter.  Shearing distorts the input by skewing it along the x or y axis.  This can cause lines to 
be slanted or at odd angles.  All of these factors can be applied at small values to fool the classifier while still appearing visualy similar.  These
methods are all fairly simple to implement while providing great results that fool the classifier.  This attack is very useful when there is a specific 
outcome in mind which was the case in this project.  In conclusion, the reasons this model was selected were its ease of implementation regardless of 
prior knowledge and its ability to be applied in a way that has a specific output in mind.

Boundary Attack
Justification - The boundary attack is an adversial attack that aims to find the decision boundary of the classifier. "It has superior effects in scenarios
where only the input image label is obtained, and it is simple, easy to implement and robust" ((Xiang et al., 2021)). It does this by feeding the classifier 
pertrubed images until it finds this boundary.  Once it is found the algorith will modify a small percentage of the pixels (1%) to produce an adversial
image that is misclassified.  The attack selects a starting point and then perturbs in a random direction.  If this still results in correct classification,
then the perturbation is increased.  If it is incorrectly classified, then the pertrubation is decreased.  The algorithm will continue to query the 
classifier until the boundary is found, and then modify pixels within that boundary.

We chose the boundary method as one of our methods for many reasons.  Boundary attacks do not require any knowledge of the classifier, but only need to
see the images and run tests on them.  This means they can easily be run on any system with very little work and are able to be used in most scenarios.
Another advanatge is they can fool the classifier with very small perturbations.  This is useful for the rubric we were provided as only 1% of the pixels
could be changed.  Because this attack finds the decision boundary it can find a small region where changes will make the most difference.  This allows the
model to make small changes that are not very noticeable, but fool the classifier.  Boundary attacks are also adaptive in their ability to find the most
effective areas of the picture to change.  It can avoid the areas of the image that do not have much factor on the classification in order to maximize
perturbation in the areas that will most likely result in a misclassification.  Overall, boundary attacks are effective due to their ease of implementation,
their effectiveness while making few changes, and adaptability.

Sources
Breaking Deep Learning with Adversarial examples using Tensorflow. (2018, May 22). CV-Tricks.com. 
https://cv-tricks.com/how-to/breaking-deep-learning-with-adversarial-examples-using-tensorflow/

Xiao, C., Zhu, J.-Y., Li, B., He, W., Liu, M., & Song, D. (n.d.). Published as a conference paper at ICLR 2018 SPATIALLY TRANSFORMED ADVERSARIAL EXAMPLES. 
Retrieved April 28, 2023, from https://openreview.net/pdf?id=HyydRMZC-
‌
‌Xiang, F., Xu, J., Zhang, W., & Wang, W. (2021). A Distributed Biased Boundary Attack Method in Black-Box Attack. Applied Sciences, 11(21), 10479. 
https://doi.org/10.3390/app112110479

Correctness Proof for FGSM code

This code takes the pre-trained model and by using an epsilon value of 1% perturbs pixels in the dandelion and grass models in order to make them be
misclassified.  It does this through an FGSM attack by making small perturbations in one step.

1. The code loads the pre-trained model
2. The code defines the size of images and the color channels
3. The code then defines epislon as 0.01 or 1%
4. The code defines a function 'fgsm_attack' that performs the FGSM attack.  This function takes the input, the label(grass or dandelion), epsilon, and 
the pre-trained model.
5. The function converts the image into an array.
6. The pixels in the array are scaled to the range [0.1].
7. The function computs the loss and gradients of the image for either grass or dandelions.
8. The sign of the gradients is computed as well as the perturbation.
9. The perturbation is applied in the range [0,1] and then the pixels are rescaled to [0,255].
10. The array is converted back to an image.
11. The adversial image is returned.
12. The mod_image function modifies some pixels.
13. The function calculates the pixels in the image in order to know how many to modify.
14. The function chooses pixels to modify and adds a value between -10 and 10 to their value.
15. The function returns the modified image

Correctness Proof for Boundry Attack code

This code loads a pre-trained image classification model and uses a boundary attack to generate adversial images for the grass and dandelions.  The code
starts with an original image of either grass or dandelions, computes the gradient of the loss, pertrubs the image in the direction of the gradient, and
checks if the image is misclassified.  If it is it ends, if not it reruns.

1. The code loads the pre-trained model.
2. The code defines the binary labels: grass as 2 and dandelion as 1.
3. The code defines the maximum percentage of pixels that can be changed which is set to 0.01 meaning 1% can be altred.
4. An image is loaded from the file containing all grass images.
5. Perform a loop that has a maximum of 100 iterations.
6. Compute the gradient of the loss.
7. Compute the step to perturb the image and perterb it by addidnt 'step' to 'x'.
8. Make sure the pixel values of 'x' are between 0 and 1.
9. Check if the classifier has been fooled by predicting if the image is grass or dandelion.
10. If the prediction is 'dandelion_label' break the loop.
11. An image is loaded from the file containing all dandelion images.
12. Perform a loop that has a maximum of 100 iterations.
13. Compute the gradient of the loss.
14. Compute the step to perturb the image and perterb it by addidnt 'step' to 'x'.
15. Make sure the pixel values of 'x' are between 0 and 1.
16. Check if the classifier has been fooled by predicting if the image is grass or dandelion.
17. If the prediction is 'grass_label' break the loop.
