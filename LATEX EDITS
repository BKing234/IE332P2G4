Important things to put in the report.
1. Justify selection of algortihms
2. Correctness proofs on code
3. Complexity analysis of overall algorithm
4. Justify efficiency tradeoffs in light of performance
5. Document all iterations and why that path was chosen
6. Appendices must include
  i. Testing/Correctness/Verification (1 section)
  ii. Runtime Complexity and Walltime (1 section)
  iii. Performance (according to the aforementioned criterion)
  iv. ustification for the algorithm selected in your final implementation out of the set of implementations you tested
7. explore the problem and solution space, and report any ideas, interesting insights, and rationale you uncover for tackling the problem
8. Any useful figures, plots, tables, or analyses are allowed to illustrate your level of depth at which you thought about the problem and your solutions.
9. Make sure all code is well-commented

Weigher Algorithm
Justification - The 'adversarial' function is designed to modify an original image input using five different adversarial image modifiers. The function
takes two parameters: the 'image' which specifies the file path of the input image, and the 'budget' specifying the maximum percentage of pixels that can
be modified by the adversarial image modifier algorithms. The 'adversarial' function first defines the weights for each image modifier respectively. In
this code, the weights are predefined to 0.2, 0.3, 0.1, 0.2, and 0.2. The function then adjusts the input image to resize it to the specified target size,
then converts it to an array to be processed and analyzed by a pre-trained machine learning model. The model makes a prediction of what that input image is
based on the highest probability. Next, the function initializes two variables to track the number of pixels used and the modified image. The 'used_pixels' 
variable keeps track of the number of pixels that have been used, and the 'modified_image' variable holds a copy of the input image that is then modified with 
the adversarial algorithms. The function then enters a loop iterating through each adversarial modifier. For each modifier, the expected number of pixels is
calculated based on the budget and current algorithm's weight, rounded to the nearest pixel. If there is enough pixels left to use within the budget, the modified
image is applied to the modifier through the 'apply_modifier' function. The 'apply_modifier' function serves to select the correct algorithm based on the
'algorithm' parameter, apply the 'image', and return the modified image. After applying the adversarial modifier, the number of used pixels and the weights
are updated. The loop continues until either all the algorithms have modified the image or the budgeted pixels are exhausted.

This algorithm was built keeping several things in mind. 
- Weights are predefined based on the how effective and computationally efficient each method was comparatively. These decisions were based on the numerous
sources that detailed the application methods and available applications of code that could beapplied to this project, which will be detailed throughout this report.
- Predictions are based on highest probability because the alternative is to hardset probabilities which introduces chances of arbitrary cases where an image
can be classified as both or neither categories. By basing on highest probability, the code makes best use of the information given, even if the confidence is not 
very high. It may also be possible that the highest probability aligns with the correct category, making a hard threshold unnecessary.
- The budget is designed to prevent excessive modifications to the image by setting a hard limit on the total number of pixels that can be modified. This ensures
that the image keeps visual integrity to the original while still causing misclassification by the model. Without the budget constraint, a large amount of pixels
could be adjusted, resulting in a heavily modified image from the original which looses its real-world applications and value.
- Weights are updated after each algorithm iteration to ensure that each modifier has a fair opportunity to modify the image and the budget is distributed
in a way that maximizes effectiveness overall. After each algorithm is applied, the remaining budget is distributed more evenly among remaining algorithms,
helping to avoid situations where a more computationally heavy algorithm gets a disproportionately large budget. This also allows the algorithm with the
highest probability of success to recieve the most budget.

FGSM algorithm
Justification - The Fast Gradient Sign Method (FGSM) is a method for generating adversial attacks that is simple yet effective.  "FGSM is a single step
attack, ie.. the perturbation is added in a single step instead of adding it over a loop (Iterative attack)" (Breaking Deep Learning with Adversarial
Examples Using Tensorflow, 2018). The method works by calculating the gradient of the loss with respect to the original image.  The algorithm then adds a 
perturbation to to the image based on the gradient's sign.  The algorithm modifies the image into its adversial version by making small perturbations.  
This allows the adversial image to look very similar to the original image while still fooling the computer.  The way this is accomplished is by finding 
how much each pixel contributes to the loss.  This is a quick process because the algorithm will follow the chain rule in finding pixels that contribute 
to maximum loss.  The chain rule breaks the computation into smaller parts, making the calculation of the gradient more accurate and efficient.  The first
step in this method is to feed the model input data and then calculate the loss.  The gradient with respect to the loss is then calculated.  From this 
gradient a perturbation is added in order to generate the new adversial model.  

We chose this method for many reasons.  The first reason is the computational efficiency of the FGSM method.  It
can generate adversial attacks on a model quickly.  This method is also easy to implement because it requires a calculation of the gradient of the loss
and then addition or subtraction of perturbations based on the sign of the gradient.  Another reason we chose to implement this method is due to its 
adaptability.  The FGSM method can be used for any machine learning model and therefore we knew it would be able to work on the binary image classifier.
The method has been proven very effective on a wide range of models.  Overall, the reason this model was selected was due to its basic implementation,
computational efficiency, and effectiveness.


Spatial-Transformation Based Attack
Justification - The Spatial-Transformation Based Attack (STBA) is an adversial attack that applies a spatial transformation to the input data in order to
create an adversial image.  "All the existing approaches directly modify pixels, but we aim to smoothly change the geometry of the scene while keeping the
original appearance, producing more perceptually realistic adversarial examples" ((Xiao et al., n.d.)).  The goal of this attack is to make the change
unrecognizable to the human eye, but still make the classifier misclassify the image.  The first step of STBA is to select a transformation function.

Possible transformations include rotating, scaling, or shearing the image.  These transformations are applied to the input and an output is produced with
features to trick the model.  

There are multiple reasons we chose this method.  The first reason is that that this model can be applied to any image without knowledge of the model.  The
attack applies a transformation until the classifier is fooled.  This means the algorthm requires much less learning and can trick the binary classifier
with more ease compared to other methods.  Another reason this model is very useful is it can be applied in a way to target specific labels that
are supposed to be misclassified.  This is useful because the only two labels were grass or dandelions, so the model could be trained appropriately
to model grass as dandelions and dandelions as grass.  You can modify the input image to look like something else and because there were only two options
this is not a hard task to implement. The ways we chose to implement were through rotating, scaling, and shearing.  Rotating the image by a certain angle
can create different featurs such as lines or curves.  Scaling the image involves expanding or shrinking dimensions along the x or y axis.  This can alter
certain defining features by making them narrower or shorter.  Shearing distorts the input by skewing it along the x or y axis.  This can cause lines to 
be slanted or at odd angles.  All of these factors can be applied at small values to fool the classifier while still appearing visualy similar.  These
methods are all fairly simple to implement while providing great results that fool the classifier.  This attack is very useful when there is a specific 
outcome in mind which was the case in this project.  In conclusion, the reasons this model was selected were its ease of implementation regardless of 
prior knowledge and its ability to be applied in a way that has a specific output in mind.

Boundary Attack
Justification - The boundary attack is an adversial attack that aims to find the decision boundary of the classifier. "It has superior effects in scenarios
where only the input image label is obtained, and it is simple, easy to implement and robust" ((Xiang et al., 2021)). It does this by feeding the classifier 
pertrubed images until it finds this boundary.  Once it is found the algorith will modify a small percentage of the pixels (1%) to produce an adversial
image that is misclassified.  The attack selects a starting point and then perturbs in a random direction.  If this still results in correct classification,
then the perturbation is increased.  If it is incorrectly classified, then the pertrubation is decreased.  The algorithm will continue to query the 
classifier until the boundary is found, and then modify pixels within that boundary.

We chose the boundary method as one of our methods for many reasons.  Boundary attacks do not require any knowledge of the classifier, but only need to
see the images and run tests on them.  This means they can easily be run on any system with very little work and are able to be used in most scenarios.
Another advanatge is they can fool the classifier with very small perturbations.  This is useful for the rubric we were provided as only 1% of the pixels
could be changed.  Because this attack finds the decision boundary it can find a small region where changes will make the most difference.  This allows the
model to make small changes that are not very noticeable, but fool the classifier.  Boundary attacks are also adaptive in their ability to find the most
effective areas of the picture to change.  It can avoid the areas of the image that do not have much factor on the classification in order to maximize
perturbation in the areas that will most likely result in a misclassification.  Overall, boundary attacks are effective due to their ease of implementation,
their effectiveness while making few changes, and adaptability.

Sources
Breaking Deep Learning with Adversarial examples using Tensorflow. (2018, May 22). CV-Tricks.com. 
https://cv-tricks.com/how-to/breaking-deep-learning-with-adversarial-examples-using-tensorflow/

Xiao, C., Zhu, J.-Y., Li, B., He, W., Liu, M., & Song, D. (n.d.). Published as a conference paper at ICLR 2018 SPATIALLY TRANSFORMED ADVERSARIAL EXAMPLES. 
Retrieved April 28, 2023, from https://openreview.net/pdf?id=HyydRMZC-
‌
‌Xiang, F., Xu, J., Zhang, W., & Wang, W. (2021). A Distributed Biased Boundary Attack Method in Black-Box Attack. Applied Sciences, 11(21), 10479. 
https://doi.org/10.3390/app112110479

Appendices

Testing/Correctness/Verification

Correctness Proof for Weigher Algorithm

\begin{enumerate}
\item Weights of the 5 algorithms are defined.
\item Loads the image from the file path according to the size set by the model.
\item The image is converted to an array.
\item The pixels in the array are scaled to [0,1] range.
\item A prediction is made based on the pretrained model.
\item A label is created marking which category the image belongs to based on highest probability.
\item The number of used pixels are defined as 0.
\item The image to be modified is loaded with size according to the model and converted to an array with defined dimensions.
\item The expected budget is defined based on the budget and weight of the current adversarial modifier.
\item Image to be modified is sent to the appropriate function corresponding to adversarial algorithms that modify the image.
\item The modified image is returned.
\item Number of used pixels are updated according to the number of pixels used.
\item Weights are updated based on which algorithm was used.
\item The function stops if the budget is exceeded and returns the modified image.
\end{enumerate}

Correctness Proof for FGSM code

This code takes the pre-trained model and by using an epsilon value of 1% perturbs pixels in the dandelion and grass models in order to make them be
misclassified.  It does this through an FGSM attack by making small perturbations in one step.

\begin{enumerate}
\item The code loads the pre-trained model
\item The code defines the size of images and the color channels
\item The code then defines epsilon as 0.01 or 1%
\item The code defines a function 'fgsm_attack' that performs the FGSM attack.  This function takes the input, the label(grass or dandelion), epsilon, and 
the pre-trained model.
\item The function converts the image into an array.
\item The pixels in the array are scaled to the range [0.1].
\item The function computs the loss and gradients of the image for either grass or dandelions.
\item The sign of the gradients is computed as well as the perturbation.
\item The perturbation is applied in the range [0,1] and then the pixels are rescaled to [0,255].
\item The array is converted back to an image.
\item The adversial image is returned.
\item The mod_image function modifies some pixels.
\item The function calculates the pixels in the image in order to know how many to modify.
\item The function chooses pixels to modify and adds a value between -10 and 10 to their value.
\item The function returns the modified image
\end{enumerate}

Correctness Proof for Boundry Attack code

This code loads a pre-trained image classification model and uses a boundary attack to generate adversial images for the grass and dandelions.  The code
starts with an original image of either grass or dandelions, computes the gradient of the loss, pertrubs the image in the direction of the gradient, and
checks if the image is misclassified.  If it is it ends, if not it reruns.

\begin{enumerate}
\item The code loads the pre-trained model.
\item The code defines the binary labels: grass as 2 and dandelion as 1.
\item The code defines the maximum percentage of pixels that can be changed which is set to 0.01 meaning 1% can be altred.
\item An image is loaded from the file containing all grass images.
\item Perform a loop that has a maximum of 100 iterations.
\item Compute the gradient of the loss.
\item Compute the step to perturb the image and perterb it by addidnt 'step' to 'x'.
\item Make sure the pixel values of 'x' are between 0 and 1.
\item Check if the classifier has been fooled by predicting if the image is grass or dandelion.
\item If the prediction is 'dandelion_label' break the loop.
\item An image is loaded from the file containing all dandelion images.
\item Perform a loop that has a maximum of 100 iterations.
\item Compute the gradient of the loss.
\item Compute the step to perturb the image and perterb it by addidnt 'step' to 'x'.
\item Make sure the pixel values of 'x' are between 0 and 1.
\item Check if the classifier has been fooled by predicting if the image is grass or dandelion.
\item If the prediction is 'grass_label' break the loop.
\end{enumerate}

Correctness Proof for Gaussian Code.

This code loads an image, and adds Gaussian noise to it.  It takes random variables with a mean 0 and standard deviation 1.  It multiplies these values
by 0.5 and then adds them to each pixel to blur the image.  The original and blurred image are then shown side by side.
\begin{enumerate}
\item The code loads an image from the model and assigns it to the variable 'A'.
\item The code adds Gaussian noise to the image and assigns it to the variable 'A.noisy'.
\item The two images are put in a two-row grid.
\item 'isoblur' function is applied with a radius of 5 to blur the noisy image.
\item The images are plotted next to each other with a title.
\end{enumerate}

Correctness Proof for Spatial-Transformation Based Attack

This code loads a pre-trained model to classify an image.  The image is transformed using scale, roation, and shear perimeters.  The transformed image is
processed and used to make a prediction to fool th classifier.  The prdicted probabilites for the original and transformed images are printed.

\begin{enumerate}
\item The required libraries are loaded.
\item The code loads the pre-trained model.
\item The code defines the size and color channel.
\item The coad loads a test image.
\item The scale, rotation, and shear parameters are defined.
\item The transformation is applied using 'imager::affine'.
\item The transformed image is reshaped and put back into the model using 'imagenet_preprocess_input'.
\item The 'predict' function is used to classify the image as dandelion or grass.
\item The predicted probabilites for the original and transformed images are printed.
\end{enumerate}

\noindent Correctness Proof for One-Pixel Attack:

This code transformed an image by creating a yellow mask that replaced all of the yellow pieces to green. 
The predicted probabilities for the original and transformed images are printed.

\begin{enumerate}
\item The required libraries are loaded.
\item The code loads a sample image.
\item The code displays the original image.
\item The code extracts the color channels of red, green and blue.
\item Then, a yellow mask is created, that is the same size as the original image, and indicates yellow pixels.
\item This yellow mask is then also displayed.
\item Next, the yellow pixels are set to green.
\item All of the channels are then recombined to display a new image where yellow is replaced with green.
\item The final, modified image is displayed. 
\end{enumerate}

Runtime Complexity and Walltime
It is required that the modified image is produced within 10 seconds with 1% of pixels allotted. However, this doesn't necessarily mean that each modifier should
take 2 seconds each, but this can be used as an initial benchmark to measure each modifier's runtime. There needs to be a balance where algorithms with longer runtimes are
shortened enough to obtain results while allowing enough time for the other algorithms to run. This can be done with the budget and the order the modifiers are run
in the weigher algorithm. Priority will be given to the algorithms that balance accuracy and runtime best together, which should yield balanced results
whether the remaining modifiers can run or not. Depending on the runtime of the more accurate algorithms, the remaining modifiers would be sorted keeping computational
time as priority. This order would also be used for any other budget range, as it manages the balance of performance and runtime reasonably. It is expected that performance
may not be a high as it could be, but it is still high enough to yield positive results in most cases. 

Performance

Training, Evaluations, Knowledge, and Accuracy

A one-pixel attack does not involve training a new model or image, but it evaluates the robustness of the pre-trained model. The availability of the 
training data is also not necessary, as this attack does not require access to any sort of data, besides the preexisting model. This means that the 
training time of this method is negligible. The evaluation time on the other hand tends to be relatively fast, as the modification of only one pixel is 
minimal. The knowledge representation, or the structure of this model, is not too important as the attack is solely based on modifying the color of a 
single pixel in an image. Lastly, the expected accuracy is typically lower than some other attack methods. This is because of the limited number of pixels
being modified. A study showed, “being able to launch non-targeted attacks by only modifying one pixel on three common deep neural network structures 
with 68.71%, 71.66% and 63.53% success rates.” (Su et al., 2019) This is still a high success rate, as one is trying to fool a neural network.  

A FGSM also does not require training of models or availability of training data, similar to one-pixel. The evaluation time is relatively fast compared to
some other adversarial attack methods. This can often be effective in a matter of seconds or less. The simplicity of the model, as it requires minimal 
steps,  leads to this conclusion. The knowledge representation is important, as the gradient of the loss function needs to be computable. FGSM attacks 
are generally considered to be a strong form of attack, which leads to a relatively high expected accuracy. Using a sample version of a FGSM, it was 
reported that “our script has obtained 99.25% accuracy on our training set and 98.77% accuracy on the testing set, implying that our model is doing a 
good job.”  (Rosebrock, 2021) This shows it is an accurate model, overall. 

A boundary attack method (BAM) does not training of a target model. Each of these models only requires access to the target model being used for the 
adversarial attack. This means the availability of additional training data is minimal or not required. The knowledge representation of the BAM relies 
on the smoothness of the boundaries and its sensitivity to changes that may occur. The BAM is considered strong and reliable, as they typically achieve 
high success rates. One study evaluated BAM on different neural network models and reported success rates ranging from 93.5% to 99.9% on the MNIST dataset, 
which contained images of handwritten digits.  (Geirhos et al., 2018)

The Gaussian Noise Attack method has an evaluation time that is relatively low, as it is much less complex than other methods, such as a boundary attack. 
This is why it is a fast operation because it does not require many tools or alteration to be successful. There is some knowledge that should be gained 
for the structure, as it relies on values for standard deviation and mean. The attack requires the image to be in a specific format (pixel values between 
0 and 1), where changes can affect the accuracy of the model. As we jump into accuracy, this form of attack tends to be weaker than some of the others 
provided. This is mainly because it is so simple and may not be sufficient to mislead the model in some cases. 


TutorMaster, A. I. (2023, February 2). What is gaussian noise in deep learning? how and why it is used? Medium. Retrieved April 27, 2023, from https://ai.plainenglish.io/what-is-gaussian-noise-in-deep-learning-how-and-why-it-is-used-af3730449e3a 

Zhao, C., \& Li, H. (2020, December 21). Blurring fools the network -- adversarial attacks by feature peak suppression and Gaussian blurring. arXiv.org. Retrieved April 27, 2023, from https://arxiv.org/abs/
2012.11442 

Su, J., Vargas, D. V., \& Kouichi, S. (2019, October 17). One pixel attack for fooling deep neural networks. arXiv.org. Retrieved April 27, 2023, from https://arxiv.org/abs/1710.08864 

Wang, P., Cai, Z., Kim, D., \& Li, W. (2021, February 23). Detection mechanisms of one-pixel attack. Wireless Communications and Mobile Computing. Retrieved April 27, 2023, from https://www.hindawi.c
om/journals/wcmc/2021/8891204/ 

Breaking Deep Learning with Adversarial examples using Tensorflow. (2018, May 22). CV-Tricks.com. 
https://cv-tricks.com/how-to/breaking-deep-learning-with-adversarial-examples-using-tensorf
low/

Xiao, C., Zhu, J.-Y., Li, B., He, W., Liu, M., \& Song, D. (n.d.). Published as a conference paper at ICLR 2018 SPATIALLY TRANSFORMED ADVERSARIAL EXAMPLES. 
Retrieved April 28, 2023, from https://openreview.net/pdf?id=HyydRMZC-
‌
‌Xiang, F., Xu, J., Zhang, W., \& Wang, W. (2021). A Distributed Biased Boundary Attack Method in Black-Box Attack. Applied Sciences, 11(21), 10479. 
https://doi.org/10.3390/app112110479

Rosebrock, A. (2021, April 17). Adversarial attacks with FGSM (fast gradient sign method). PyImageSearch. Retrieved April 28, 2023, from https://pyimagesearch.com/2021/03/01/adversarial-attacks-with-fgsm-fast-gradient-sign-method/ 

Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F. A., \& Brendel, W. (2018, September 27). ImageNet-trained cnns are biased towards texture; increasing shape... OpenReview. Retrieved April 28, 2023, from https://openreview.net/forum?id=Bygh9j09KX 

