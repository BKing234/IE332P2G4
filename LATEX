Important things to put in the report.
1. Justify selection of algortihms
2. Correctness proofs on code
3. Complexity analysis of overall algorithm
4. Justify efficiency tradeoffs in light of performance
5. Document all iterations and why that path was chosen
6. Appendices must include
  i. Testing/Correctness/Verification (1 section)
  ii. Runtime Complexity and Walltime (1 section)
  iii. Performance (according to the aforementioned criterion)
  iv. ustification for the algorithm selected in your final implementation out of the set of implementations you tested
7. explore the problem and solution space, and report any ideas, interesting insights, and rationale you uncover for tackling the problem
8. Any useful figures, plots, tables, or analyses are allowed to illustrate your level of depth at which you thought about the problem and your solutions.
9. Make sure all code is well-commented

ONE-PIXEL ATTACK METHOD

There are many types of algorithms and attacks that can be used to fool an image classification model. One model used to do so is the one-pixel 
attack method. According to a published research journal called ‚ÄòHindawi‚Äô, ‚Äúthe purpose of a one-pixel attack is to maliciously change the 
classification result of a victim image from its original label to a target lab.‚Äù (Wang et al., 2021) In more condense terms, this means a single 
pixel is modified in an input image causing a misclassification by the model. This small change of only one pixel may not be noticeable to the human
eye but can be just enough to trick the classifier. 

We chose to use this model because it is simple and efficient. Using the one-pixel attack method, an attacker modifies one pixel of the image. Take 
our images, for example. A small change within a dandelion or the background color in a piece of grass, could cause our classier to display incorrect
results. This was shown in the pixel code by creating a yellow mask and replacing the color yellow with the color green. In this case, replacing 
pieces of a dandelion with the green color could result in the classifier mistaking it for grass. 

Another reason to use the one-pixel attack method is because it is typically difficult to detect. In many cases, especially when trying to trick a 
form of AI, it is best if it is not obvious the image has been altered. In our case, it is more noticeable that the image has been altered to 
guarantee there were changing being made and tests to be run. In the perfect scenario, it would be unknown that any changes were made to the image, 
but the classifier displays an incorrect final assumption percentage. 

Another benefit to using one-pixel attacks is they typically have a high success rate. Through testing and many modifications, we did not have the 
ability to test this model to its full potential, so this statement was not able to be verified. Because of this, further research was performed to 
gage a better understanding of what should have occurred within this attack. This led to knowledge regarding this sensitivity to small changes within
images, including the color values of individual pixels. The lack of strength and flexibility within these models to accept changes ultimately makes
the susceptible to adversarial attacks. (Su et al., 2019) This is why the success rates of one-pixel attacks are so high, as they are baffling in 
the eyes of the classifier. 

Throughout this project, there was more success working with this type of algorithm in Matlab. The code and images provided were the results of a 
Matlab code, but displayed errors when attempted to run in R. The R code failed to alter the image in any way, which was assumed to be because of 
the lack of determination when changing singular pixels. This was not able to be determined, as we were unable to run the code any further in R form. 
In order to show that something was done to attempt this, we created a larger change in the colors of a sample image in Matlab. Both the Matlab and R 
version of the code will be provided to show the process and understanding of the attack attempted to be made. The image below shows the original 
photo, the yellow mask of the photo, and the photo after it had been changed.  From this, it is shown that the correctness of the code is minimal, 
but the overall algorithm did make changes to an image to fool a classifier based on the colors. If there were more time, knowledge, and contribution
, the efficiency in the light of performance would have increased from where it is now. It is difficult to express the performance of the algorithm, 
as it is lacking functionality. 

GAUSSIAN TECHNIQUE


The next technique used to fool an image classifier is inserting noise to output a blurred version of the original image. This was found using the 
Gaussian attack method. Adding noise disrupts patterns, details, and other factors that make an image recognizable. This results in the image being more 
difficult to distinguish and classify for the model.  Research done at Cornell University found: ‚ÄúBased on the blurring spirit of PS, we further apply 
gaussian blurring to the data, to investigate the potential influence and threats of gaussian blurring to performance of the network. Experiment results 
show that PS and well-designed gaussian blurring can form adversarial attacks that completely change classification results of a well-trained target 
network.‚Äù (Zhao & Li, 2020)

The reasoning for using this tactic was because it was effective, flexible, and easy to use. With the troubles our team has faced within the project, we 
search for an alternative that would change the image in little to no time, using limited functions and tools within R. Through this, we decided on the 
Gaussian method to add noise to the image. This is effective because the image becomes foggy and less distinct, which creates confusion for the classifier.
Our prediction, because we failed in the functionality portion of this process, is that this technique would be most effective when blurring a dandelion.
This would confuse the classifier by displaying an image with less details, such as the photos of the grass. 

The Gaussian attack method is one of the most widely used algorithms. This is used by adding Gaussian noise, which is mathematically defined as 
ùë•‚Ä≤=ùë•+ùúéùëÅ(0,1) where x is the unmodified image, x‚Äô is the modified image, ùúé is the standard deviation, and N(0,1) is a noise distribution with a mean of 0 
and standard deviation of 1. (TutorMaster, 2023) The amount of noise that gets added is controlled by the standard deviation, where the small this value 
is, the less noticeable it is. Although this is effective in tricking a human, it may not be as effective when fooling a classifier. This is why testing is
important to reach this middle point and use the most practical method for both factors. 

To create a better understanding and show we attempted to process the images, we created a breakdown of our code and its functionality. This code is loading 
an image from a folder and plating an unmodified version to start. This stores it in a variable called ‚ÄòA‚Äô. The next line creates a modified version of 
the original image by adding noise to each pixel value of the original image. The ‚Äòrnorm()‚Äô function generates a set of random numbers and ‚Äòprod(dim(A))‚Äô 
calculates the total number of pixels in the image to ensure there is enough random noise generated to match the size of the original image. This is 
stored as a new variable labeled ‚ÄòA.noisy‚Äô. Finally, the ‚Äòisoblur()‚Äô function is applied to the noisy image to created a blurred version. This applies 
to a Gaussian blur filter that smooths and sharpens features in the image. Below are the two images displayed in result of the code provided.

The correctness of our proofs on the code is limited to what runs in the R code, but not within our overall algorithm. The code used displays an image 
with noise that is blurred to create a modified image. Because there is not enough to back up the percentages in the algorithm to show if this tactic 
works, there is not much more to comment on. Our prediction is that it would be successful based on the research completed for this method. This leads 
into the complexity of the overall algorithm, as it did not display results. The most that can be said is that this would typically be a reliable algorithm
would work to fool an image classification model. 


TutorMaster, A. I. (2023, February 2). What is gaussian noise in deep learning? how and why it is used? Medium. Retrieved April 27, 2023, from https://ai.plainenglish.io/what-is-gaussian-noise-in-deep-learning-how-and-why-it-is-used-af3730449e3a 
Zhao, C., & Li, H. (2020, December 21). Blurring fools the network -- adversarial attacks by feature peak suppression and Gaussian blurring. arXiv.org. Retrieved April 27, 2023, from https://arxiv.org/abs/2012.11442 
Su, J., Vargas, D. V., & Kouichi, S. (2019, October 17). One pixel attack for fooling deep neural networks. arXiv.org. Retrieved April 27, 2023, from https://arxiv.org/abs/1710.08864 
Wang, P., Cai, Z., Kim, D., & Li, W. (2021, February 23). Detection mechanisms of one-pixel attack. Wireless Communications and Mobile Computing. Retrieved April 27, 2023, from https://www.hindawi.com/journals/wcmc/2021/8891204/ 

